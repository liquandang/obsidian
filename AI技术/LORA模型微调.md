## 一、为什么要做模型微调 (The "Why")

预训练大模型（如 Llama、Qwen、GPT-4 等）是在海量通用数据上训练出来的“通才”，它们知识渊博，但对于特定场景和任务，往往存在以下不足，需要通过在特定数据上的“再学习”，改造为一个高效、可靠、安全的“专才”模型。

### 1、企业对于大模型的不同类型个性化需求

- 提高模型对企业专有信息的理解、增强模型在特定行业领域的知识 - **SFT**
	- 案例一：希望大模型能更好理解酒店OTA企业专有知识，并熟练回答关于汉堡行业的所有问题，如什么是酒店LDBO页，什么是定价、促销等
- 提供个性化和互动性强的服务 - **RLHF**
    - 案例二：希望大模型能够基于顾客的反馈调整回答方式，比如生成更二次元风格的回答还是更加学术风格的回答
- 提高模型对企业专有信息的理解、增强模型在特定行业领域的知识、获取和生成最新的、实时的信息 - **RAG**
    - 案例三：希望大模型能够实时获取酒店OTA的最新的促销活动信息更新

### 2、SFT (有监督微调)、RLHF (强化学习)、RAG (检索增强生成)

#### 2.1、SFT (Supervised Fine-Tuning) 有监督微调**
- 定义：
	- 通过提供人工标注的数据，进一步训练预训练模型，让模型能够更加精准地处理特定领域的任务
	- 除了“有监督微调”，还有“无监督微调”“自监督微调”，当大家提到“微调”时通常是指有监督微调
- 通俗解释：
	
	**有监督微调（SFT）** 就好比你这个**主管亲自带教**的过程。
	你会拿出一本《优秀客服邮件范例大全》（这就是**高质量的标注数据集**），然后对他说：
	
	- **你看，当客户问“我的订单什么时候发货？”（这是instruction），你应该这样回答：“您好！您的订单正在加急处理中，预计24小时内发货，请您耐心等待。”（这是output）。**
	    
	- **当客户问“如何退款？”（instruction），你应该这样回答：“您好！我们支持7天无理由退货，请您在订单页面点击‘申请售后’即可，我们会尽快为您处理。”（output）。**
	
	你给他看了成百上千个这样的“问题-标准答案”范例后，这个实习生慢慢就“上道”了。他不仅学会了知识，更重要的是，他学会了**如何按照你期望的特定格式、语气和风格**来运用他的知识进行沟通。
	
	**总结一下：SFT的核心就是“模仿”。它通过提供大量高质量的“指令-回答”对，让模型学会如何针对特定类型的指令，生成符合我们预期的、特定格式或风格的回答。**
- 有监督微调用来干什么？
	1. **让模型“听懂人话”：** 基础模型只会续写，SFT教会它理解并遵循指令，这是所有对话模型的第一步。
	2. **注入特定风格或角色：** 让模型模仿某种写作风格（如古龙、鲁迅），或者扮演一个角色（如健身教练、法律顾问）。
	3. **遵循特定输出格式：** 教会模型生成固定格式的文本，比如JSON、XML、Markdown表格等，这在程序调用中非常重要。
	4. **掌握特定领域的对话模式：** 让模型成为一个特定领域的“专家客服”，知道该领域的常用术语和沟通范式。
- 微调过程发生了什么？
	1. **输入：** 训练程序会把 instruction 和 input 拼接起来，喂给大模型。例如，模型会看到：“你是一个热情的小笼包餐厅客服，请回答顾客的问题。你们店的招牌菜是什么？”
	2. **预测：** 模型根据它已有的知识，生成一个自己的回答。比如它可能回答：“小笼包是一种源自中国上海的著名点心。” —— 这个回答虽然正确，但完全不符合“客服”的身份。
	3. **计算差距（损失）：** 系统会比较模型的回答和我们提供的output（“您好！我们店的招牌是...”）之间的差距。这个差距越大，“损失”（Loss）就越大。
	4. **学习和调整：** 模型会根据这个“损失”，反向调整自己内部的无数个参数（权重），目标是下一次再遇到类似问题时，生成的回答能更接近我们的“标准答案”。
	5. **重复：** 这个过程会在整个数据集上重复很多遍。
- 微调示例
	- **方法一：instruction 作为“系统指令/角色设定” (推荐用于聊天机器人)
		这种方法最适合用来微调一个**有特定人设、需要进行多轮对话的聊天机器人**。
		- **instruction**: 定义一个**通用的、不变的背景或角色**。它告诉模型：“你接下来要扮演什么角色”或“你要完成什么类型的大任务”。
		- **input**: 存放**用户的具体、每一次都不同的问题**。
		- **output**: 模型应该给出的、符合人设的回答。
		
		**【restaurant_data.json 范例 - 角色扮演法】**
		在这种模式下，instruction 在所有相关数据条目中可以是完全相同的。
		```
		[
		  {
		    "instruction": "你是一个名为“小笼大师”的智能客服，负责热情、专业地回答顾客关于我们小笼包餐厅的各种问题。",
		    "input": "你们店的招-牌-菜是-什么呀？",
		    "output": "您好！我们“小笼大师”非常自豪地向您推荐我们的两大王牌：【蟹粉小笼包】和【经典鲜肉小笼包】。每一颗都皮薄馅大，汤汁鲜美，保证让您回味无穷！"
		  },
		  {
		    "instruction": "你是一个名为“小笼大师”的智能客服，负责热情、专业地回答顾客关于我们小笼包餐厅的各种问题。",
		    "input": "营业时间？",
		    "output": "我们每天的营业时间是上午10:00到晚上9:30，随时欢迎您的光临哦！"
		  },
		  {
		    "instruction": "你是一个名为“小笼大师”的智能客服，负责热情、专业地回答顾客关于我们小笼包餐厅的各种问题。",
		    "input": "我一个人去吃，大概多少钱？",
		    "output": "您好，我们店的人均消费大约在60-80元之间，您可以根据喜好自由搭配，性价比超高的！"
		  }
		]
		```
		
		**为什么这种方法好？**
		
		1. **人设稳定：** 模型能清晰地学习到它的“核心身份”，无论用户问什么，它都知道要保持“小笼大师”这个热情客服的身份来回答。
		2. **泛化性强：** 训练完成后，你只需要给模型一个系统指令（System Prompt）“你是一个名为‘小笼大师’的……”，然后输入任何新的用户问题，它都能很好地接住并扮演角色。这非常符合现代聊天模型的交互范式。
	- **方法二：instruction 作为“用户的具体问题” (通用任务微调)
		这种方法更像是把每一次交互都看作一个独立的“任务”，适合微调模型来**完成各种不同的、一次性的指令**。
		- **instruction**: **直接就是用户的具体问题或指令**。
		- **input**: 通常为空。或者，如果指令需要处理一段特定的文本（比如“总结以下文章”），那么这段文章就放在 input 里。
		- **output**: 模型应该给出的直接回答。
		**【restaurant_data.json 范例 - 具体任务法】**
		在这种模式下，instruction 每条都不一样，而 input 几乎总是空的。
		```
		[
		  {
		    "instruction": "你们餐厅的招牌菜是什么？",
		    "input": "",
		    "output": "您好！我们店的招牌是【蟹粉小笼包】和【经典鲜肉小笼包】，皮薄馅大，汤汁鲜美，强烈推荐您品尝一下哦！"
		  },
		  {
		    "instruction": "请告诉我你们的营业时间。",
		    "input": "",
		    "output": "我们每天的营业时间是上午10:00到晚上9:30，全天恭候您的光临！"
		  },
		  {
		    "instruction": "在你们餐厅，一个人吃饭大概要花多少钱？",
		    "input": "",
		    "output": "您好，我们店的人均消费大约在60-80元左右，丰俭由人，性价比很高的！"
		  }
		]
		```
		
		**这种方法有什么特点？**
		1. **直接明了：** 数据格式非常简单，就是“问-答”对。
		2. **任务驱动：** 模型学习的是“针对这类问题，就给出这类答案”。它的人设和风格是**隐含在所有 output 的风格中**的，而不是通过一个明确的指令来学习。
		3. **局限性：** 如果你想在推理时改变模型的角色（比如让它从客服变成诗人），会比较困难，因为它没有被明确地训练过如何遵循一个“系统角色指令”。
	- **结论与最佳实践
		![[Pasted image 20250614181721.png]]
	
		**给您的建议：**
		- **如果你的目标是打造一个“客服机器人”，强烈推荐使用【方法一】。** 这种方式训练出的模型更稳定、更“可控”，也更符合当前主流大模型（如GPT-4, Llama）的“系统-用户-助手”对话模板。
		- 如果你只是想让模型学会回答一些固定的事实性问题，或者做一些翻译、总结等一次性任务，【方法二】更简单直接。
#### 2.2、RLHF (Reinforcement Learning from Human Feedback) 强化学习**
- DPO (Direct Preference Optimization)
	 核心思想：通过人类对比选择（例如：A选项和B选项，哪个更好）直接优化生成模型，使其产生更符合用户需求的结果；调整幅度大。

- PPO（Proximal Policy Optimization）
	 核心思想：通过奖励信号来渐进式调整模型的行为策略；调整幅度小。
	![[Pasted image 20250614175326.png|449x155]]
#### 2.3、RAG (Retrieval-Augmented Generation) 检索增强生成**
- 将外部信息检索与文本生成结合，帮助模型在生成答案时，实时获取外部信息和最新信息
### 3、微调还是RAG?

- **微调:**
    - 适合: 拥有非常充足的数据
    - 能够直接提升模型的固有能力; 无需依赖外部检索;
- **RAG:**
    - 适合: 只有非常少的数据; 动态更新的数据
        
    - 每次回答问题前需耗时检索知识库; 回答质量依赖于检索系统的质量;
- **总结:**
    - 少量企业私有知识: 最好微调和 RAG 都做; 资源不足时优先 RAG;    
    - 会动态更新的知识: RAG
    - 大量垂直领域知识: 微调

## 二、LORA微调算法

### LORA论文
 
 LoRA 的开山之作 **《[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)》**，LoRA 的核心思想：低秩适应 (Low-Rank Adaptation)
 ``` 
 h = W0x + ∆W x = W0x + BAx
LORA如何工作：
	1. 冻结原始权重 W：在微调过程中，巨大的预训练模型权重W完全不参与训练，保持不变。这一下就节省了绝大部分的计算和显存。
	2. 注入旁路结构：在原始的矩阵乘法 h = Wx 旁边，增加一个“旁路” BAx。A 和 B 就是那两个低秩矩阵。
	    - A 的维度是 r × k，B 的维度是 d × r，其中 r 是一个远小于 d 和 k 的秩。
	    - 初始化时，A 采用随机高斯分布，而 B 初始化为零。这保证了在训练开始时，旁路 BA 为零，模型保持其原始性能。
	3. 只训练 A 和 B：整个微调过程，梯度只在 A 和 B 上计算和更新。由于 r 很小，可训练的参数量极少（通常是原始模型的 **0.01%** 左右）。
	4. 推理时无额外延迟：这是 LoRA 相对于其他参数高效微调方法（如 Adapter）的巨大优势。训练完成后，可以计算出 ΔW = BA，然后将其直接加回到原始权重中：W' = W + ΔW。之后，这个模型就和一个普通的、全量微调过的模型一模一样，在推理时没有任何额外的计算开销。
```

![[Pasted image 20250614183915.png|385x354]]

<mark style="background: #D2B3FFA6;">来自B站的讲解</mark>：https://www.bilibili.com/video/BV1euPkerEtL/?spm_id_from=333.1391.0.0&vd_source=eaf2780903a447c342cd81c97f62b243
![[Pasted image 20250614183315.png|504x336]]

### LORA论文关键结果与发现

论文通过大量实验证明了 LoRA 的有效性，其中有几个非常重要的发现：

<mark style="background: #FF5582A6;">**发现一：LoRA 的性能与全量微调相当，甚至更好。**  </mark>
下表展示了在 1750 亿参数的 GPT-3 上，LoRA 与其他方法（包括全量微调 FT）的性能对比。

|   |   |   |   |
|---|---|---|---|
|模型与方法|可训练参数量|WikiSQL (Acc.%)|SAMSum (RougeL)|
|GPT-3 (Full FT)|175.3 B|73.8|44.5|
|Adapter|40.1 M|73.2|45.1|
|**LoRA**|**37.7 M**|**74.0**|**45.1**|

**结论：** LoRA 用**不到 0.03%** 的参数，就在多个任务上达到了与全量微调相当甚至超越的性能。

<mark style="background: #FF5582A6;">**发现二：并非所有权重都需要LoRA，注意力权重最关键！**  </mark>
作者研究了在 Transformer 的不同权重矩阵上应用 LoRA 的效果。

|   |   |   |   |
|---|---|---|---|
|应用 LoRA 的权重|秩 (r)|WikiSQL (Acc.%)|MultiNLI (Acc.%)|
|Wq|8|70.4|91.0|
|Wk|8|70.0|90.8|
|Wv|8|73.0|91.0|
|**Wq, Wv**|**4**|**73.7**|**91.3**|
|Wq, Wk, Wv, Wo|2|73.7|91.7|

**结论：** 实验表明，仅仅在**注意力**的 Query (Wq) 和 Value (Wv) 矩阵上应用 LoRA，就能取得最佳的性能和效率平衡。这为后来的实践者提供了非常宝贵的指导。

<mark style="background: #FF5582A6;">**发现三：LoRA 的秩 r 无需很大，极小的秩就能发挥巨大作用。**  </mark>
一个自然的疑问是：秩 r 是不是越大越好？

|   |   |   |
|---|---|---|
|秩 (r)|WikiSQL (Acc.%)|MultiNLI (Acc.%)|
|1|73.4|91.3|
|2|73.3|91.4|
|4|73.7|91.3|
|8|73.8|91.6|
|64|73.5|91.4|

**惊人发现：** 秩 r 并不总是越大越好。在很多任务上，r 设置为 **1, 2, 4, 8** 这样非常小的值，就已经足够了。这强有力地证明了论文的核心假设——**模型适应的权重变化确实是低秩的**。


## 三、模型微调


